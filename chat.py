import os
import chainlit as cl
from huggingface_hub import login
from sentence_transformers import SentenceTransformer
import time
from dotenv import load_dotenv
import numpy as np

load_dotenv()
# Log in to HuggingFace (only if needed for private models)
login(token=os.getenv("HUGGINGFACE_TOKEN"))

# Load the Vietnamese-compatible sentence transformer
model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')

# Data
conversation_pairs = [
    {
        "query": "Ch√†o b·∫°n",
        "answer": "Ch√†o Thi√™n Long, ch√∫c b·∫°n bu·ªïi s√°ng vui v·∫ª! M√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?"
    },
    {
        "query": "B·∫°n c√≥ th·ªÉ l√†m ƒë∆∞·ª£c nh·ªØng g√¨ v·∫≠y?",
        "answer": "M√¨nh l√† m·ªôt h·ªá th·ªëng AI Agent v·ªõi c√°c kh·∫£ nƒÉng nh∆∞:\n\n‚Ä¢ T∆∞ v·∫•n tuy·ªÉn sinh cho Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa ‚Äì ƒêHQG-HCM\n‚Ä¢ D·ªãch ti·∫øng Vi·ªát sang ti·∫øng Bahnar\n‚Ä¢ H·ªó tr·ª£ thay ƒë·ªì ·∫£o tr√™n ·∫£nh c·ªßa b·∫°n"
    },
    {
        "query": "Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa c√≥ nh·ªØng ph∆∞∆°ng th·ª©c tuy·ªÉn sinh n√†o?",
        "answer": "Tr∆∞·ªùng hi·ªán ƒëang √°p d·ª•ng c√°c ph∆∞∆°ng th·ª©c tuy·ªÉn sinh nh∆∞ sau:\n\n1. Ph∆∞∆°ng th·ª©c 1 (TTBO): X√©t tuy·ªÉn th·∫≥ng theo quy ch·∫ø c·ªßa B·ªô GD&ƒêT, chi·∫øm kho·∫£ng 1% ‚Äì 5% t·ªïng ch·ªâ ti√™u.\n\n2. Ph∆∞∆°ng th·ª©c 2 (THOP): X√©t tuy·ªÉn t·ªïng h·ª£p, chi·∫øm kho·∫£ng 95% ‚Äì 99% t·ªïng ch·ªâ ti√™u, g·ªìm 5 nh√≥m ƒë·ªëi t∆∞·ª£ng:\n\n- ƒê·ªëi t∆∞·ª£ng 1: Th√≠ sinh c√≥ k·∫øt qu·∫£ k·ª≥ thi ƒê√°nh gi√° nƒÉng l·ª±c ƒêHQG-HCM nƒÉm 2025\n- ƒê·ªëi t∆∞·ª£ng 2: Th√≠ sinh kh√¥ng c√≥ k·∫øt qu·∫£ k·ª≥ thi ƒê√°nh gi√° nƒÉng l·ª±c\n- ƒê·ªëi t∆∞·ª£ng 3: Th√≠ sinh t·ªët nghi·ªáp THPT ·ªü n∆∞·ªõc ngo√†i\n- ƒê·ªëi t∆∞·ª£ng 4: Th√≠ sinh s·ª≠ d·ª•ng ch·ª©ng ch·ªâ tuy·ªÉn sinh qu·ªëc t·∫ø\n- ƒê·ªëi t∆∞·ª£ng 5: Th√≠ sinh ƒëƒÉng k√Ω ch∆∞∆°ng tr√¨nh chuy·ªÉn ti·∫øp qu·ªëc t·∫ø (√öc, M·ªπ, New Zealand)\n\nL∆∞u √Ω:\n\n‚Ä¢ Th√≠ sinh c√≥ th·ªÉ tham gia x√©t tuy·ªÉn theo nhi·ªÅu ƒë·ªëi t∆∞·ª£ng v√† s·∫Ω ƒë∆∞·ª£c l·∫•y ƒëi·ªÉm cao nh·∫•t ƒë·ªÉ x√©t tuy·ªÉn.\n‚Ä¢ N·∫øu c√≥ ch·ª©ng ch·ªâ IELTS ‚â• 5.0, TOEFL iBT ‚â• 46 ho·∫∑c TOEIC ƒë·ªß ƒëi·ªÅu ki·ªán, th√≠ sinh s·∫Ω ƒë∆∞·ª£c quy ƒë·ªïi ƒëi·ªÉm m√¥n ti·∫øng Anh t∆∞∆°ng ·ª©ng trong t·ªï h·ª£p x√©t tuy·ªÉn."
    },
    {
        "query": "Tr∆∞·ªùng h√® khoa h·ªçc CSE ‚Äì Summer School 2025 c√≥ ch·ªß ƒë·ªÅ g√¨ v·∫≠y?",
        "answer": "Tr∆∞·ªùng h√® nƒÉm nay s·∫Ω t·∫≠p trung v√†o ch·ªß ƒë·ªÅ AI Agents. Ch∆∞∆°ng tr√¨nh d·ª± ki·∫øn di·ªÖn ra v√†o ƒë·∫ßu th√°ng 7, d√†nh cho c√°c b·∫°n h·ªçc sinh THPT quan t√¢m ƒë·∫øn tr√≠ tu·ªá nh√¢n t·∫°o v√† ng√†nh Khoa h·ªçc M√°y t√≠nh."
    },
    {
        "query": "D·ªãch ti·∫øp c√¢u n√†y nha: 'Tr∆∞·ªùng h√® khoa h·ªçc 2025 s·∫Ω v·ªÅ ch·ªß ƒë·ªÅ AI Agents.'",
        "answer": "Tr∆∞∆°ng k∆°ti√™ng khoa h≈èk 2025 li√™m s∆∞t krƒÉ ≈≠nh ƒë√™ tak ƒëon trƒ≠ tu√™ nh√¢n tao."
    },
    {
        "query": 'B·∫°n d·ªãch gi√∫p m√¨nh c√¢u n√†y sang ti·∫øng Bahnar nh√©: "T√¥i t√™n Long, t√¥i l√† sinh vi√™n nƒÉm 2 ng√†nh Khoa h·ªçc M√°y t√≠nh t·∫°i Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa."',
        "answer": "Inh t√™n Long, inh la sinh vi√™n s√≤nƒÉm 2 ng√†nh khoa h≈èk m·ªìn h·ªçc m√°y hm·∫Øi l∆°m Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa."
    },
    {
        "query": "Thay gi√∫p tui ·∫£nh c·ªßa ng∆∞·ªùi m·∫´u n√†y v·ªõi b·ªô ƒë·ªì tui g·ª≠i nha.",
        "answer": ""
    },
    {
        "query": "C·∫£m ∆°n b·∫°n nha.",
        "answer": "Kh√¥ng c√≥ g√¨ b·∫°n. B·∫°n nh·ªõ ƒëƒÉng k√Ω tham gia CSE ‚Äì Summer School 2025 nha. üòä T·∫°m bi·ªát thi√™n long!"
    }
]

# Pre-compute embeddings for all queries
query_embeddings = []
for pair in conversation_pairs:
    embedding = model.encode(pair["query"])
    query_embeddings.append({
        "embedding": embedding,
        "answer": pair["answer"],
        "query": pair["query"]
    })


def semantic_search(query, threshold=0.7):
    # Encode the query
    query_embedding = model.encode(query)

    # Find the most similar query
    max_similarity = -1
    best_answer = None

    for item in query_embeddings:
        # Calculate cosine similarity
        similarity = np.dot(query_embedding, item["embedding"]) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(item["embedding"])
        )

        if similarity > max_similarity:
            max_similarity = similarity
            best_answer = item["answer"]

    if max_similarity >= threshold:
        return best_answer
    return "T√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ c√¢u h·ªèi n√†y. B·∫°n c√≥ th·ªÉ h·ªèi c√¢u kh√°c ƒë∆∞·ª£c kh√¥ng?"


@cl.on_chat_start
async def on_chat_start():
    await cl.Message(content="üëã Xin ch√†o! T√¥i l√† AI Agent c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc B√°ch Khoa. B·∫°n mu·ªën h·ªèi g√¨?").send()


@cl.on_message
async def on_message(message: cl.Message):
    time.sleep(3)
    query = message.content

    # Check if there are exactly 2 images attached
    if len(message.elements) == 2 and all(element.type == "image" for element in message.elements):
        time.sleep(7)
        # User sent text with 2 images, respond with the answer.jpg image
        answer_image_path = "answer.jpg"

        if os.path.exists(answer_image_path):
            # Respond with the processed image
            elements = [
                cl.Image(
                    name="processed_image",
                    display="inline",
                    path=answer_image_path
                )
            ]
            await cl.Message(content="ƒê√¢y l√† k·∫øt qu·∫£ thay ƒë·ªì ·∫£o cho b·∫°n:", elements=elements).send()
        else:
            await cl.Message(content="Kh√¥ng t√¨m th·∫•y file ·∫£nh k·∫øt qu·∫£. Vui l√≤ng th·ª≠ l·∫°i.").send()
    else:
        # Regular text-based response
        answer = semantic_search(query)

        # Typing effect (streaming token by token)
        msg = cl.Message(content="")
        await msg.send()

        for token in answer.split():
            await msg.stream_token(token + " ")
            time.sleep(0.03)  # typing speed simulation

        msg.content = answer
        await msg.update()